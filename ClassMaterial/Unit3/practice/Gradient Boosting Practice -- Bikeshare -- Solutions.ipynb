{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 12 Bonus Lab:  Gradient Boosting With the Bikeshare Dataset\n",
    "\n",
    "Welcome!  This notebook is designed to provide additional practice for people looking to get more familiar with Gradient Boosting & SciKit Learn.\n",
    "\n",
    "The topic of the notebook is using Gradient Boosting to forecast demand for bikeshare stations.\n",
    "\n",
    "The dataset has the following columns:  \n",
    "\n",
    "  - **datetime:** a timestamp collected hourly.\n",
    "  - **season:** a categorical column that lists the current season for that observation\n",
    "  - **holiday:** a column (0 or 1), that detects whether or not it was a holiday\n",
    "  - **workingday:** a column (0 or 1), that encodes whether or not it was a workday or not\n",
    "  - **weather:** a categorical column that lists a light weather description for the observation\n",
    "  - **temp:** the temperature outside\n",
    "  - **atemp:** the temperature it feels like outside\n",
    "  - **humidity:** the humidity outside\n",
    "  - **windspeed:** the windspeed, in mph\n",
    "  - **count:** the number of bikes checked out during that hour\n",
    "  \n",
    "Your job is to build a regression model that appropriately captures the information available to make the most accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Load in the Dataset\n",
    "\n",
    " - It's called `bikeshare.csv`\n",
    " - Make sure to make `datetime` a time column\n",
    " - It's not a bad idea to use it as an index column, although this isn't necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "df = pd.read_csv('../../data/bikeshare.csv', parse_dates=['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Transform Your Categorical Variables (If Necessary)\n",
    "\n",
    "This dataset has two categorical columns -- `weather` and `season`.  Decide how you might encode these -- One Hot encoding, ordinal encoding, or categorical encoding (or try several if you have time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "cat_cols = df.select_dtypes(include=np.object).columns.tolist()\n",
    "df[cat_cols] = df[cat_cols].astype('category')\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a Training & Test Set\n",
    "\n",
    "Given that there's a time based column, make the most recent values your test set.  Do a 20% split.  (You can use `train_test_split` for this, but it's not necessary.  You could also just sort by `datetime` and take the bottom 20% of rows for your test set).\n",
    "\n",
    "**Note:** You can use the argument `shuffle=False` if you want to use `train_test_split` without shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "X = df.drop(['count', 'datetime'], axis=1)\n",
    "y = df['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:8000].copy(), X[8000:].copy(), y[:8000].copy(), y[8000:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create a Validation Set From Your Training Set\n",
    "\n",
    "Remember....this is your test set within the training set.  Make it 20% of your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "X_train, X_val, y_train, y_val = X_train[:6400].copy(), X_train[6400:].copy(), y_train[:6400].copy(), y_train[6400:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Do An Initial Fitting And Scoring of Your Model\n",
    "\n",
    " - Remember, fit on the training set, and score on the validation set.\n",
    " - How much is your model overfitting (if at all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10766459062257538"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer here\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm.fit(X_train, y_train)\n",
    "gbm.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Look At Your Feature Importance Scores\n",
    "\n",
    "What seems to be having the most impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>temp</td>\n",
       "      <td>0.290335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>humidity</td>\n",
       "      <td>0.289762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atemp</td>\n",
       "      <td>0.262040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>workingday</td>\n",
       "      <td>0.056994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>season</td>\n",
       "      <td>0.051021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Features  Importances\n",
       "4        temp     0.290335\n",
       "6    humidity     0.289762\n",
       "5       atemp     0.262040\n",
       "2  workingday     0.056994\n",
       "0      season     0.051021"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer here\n",
    "feats = pd.DataFrame({\n",
    "    'Features': X.columns,\n",
    "    'Importances': gbm.feature_importances_\n",
    "}).sort_values(by='Importances', ascending=False)\n",
    "\n",
    "feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Build New Features (ie, Add New Columns To Your Dataset)\n",
    "\n",
    "This is your chance to think about ways to better capture the value and impact of time and other variables on the target variable (`count`).\n",
    "\n",
    "What you should do here is add a new feature to your training and validation set, re-run your model on the  training set, and score it on the validation set to see if it made an improvement.  \n",
    "\n",
    "A good place to start with this is extracting out different date parts to see if they improve your validation score.\n",
    "\n",
    "You can find information about the different dateparts in pandas here:  https://pandas.pydata.org/pandas-docs/version/0.24/reference/series.html#time-series-related\n",
    "\n",
    "Or if you're using the `datetime` column as an index:  https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DatetimeIndex.html\n",
    "\n",
    "Keep features if they improve your validation score, discard them if they don't.\n",
    "\n",
    "A few other ideas:  \n",
    "\n",
    " - you can create a column called `Daytime` that tests whether or not it's light outside.  (Ie, between 7PM - 7AM is `False`, `True` otherwise).  You could also get fancier and adjust the daylight hours depending on season.  \n",
    "\n",
    " - you can also create a variable that tracks the passage of time.  This can be done by finding the earliest date in the dataset, subtracting each observed date from that and extracting the datepart in days.  This way, if you have an upward or downward trend throughout the dataset, you'd be able to capture it. So something like `X_train['time'] = (X_train.index.hour - earliest_date).days` \n",
    " - You could also try multiplying different columns together.  Maybe it being `Daytime`, `Sunny` and low humidity has a multiplicative effect that isn't totally captured by any of the variables by themselves.\n",
    "\n",
    "**Note:** Dateparts, despite being numbers, are probably best thought of as **categorical** variables.....think about it -- the 11 AM hour is something distinct from the 11 PM hour.....they are best interpreted as being separate categories than one continuous column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "# we'll add in the datetime column back into our dataset and refit\n",
    "X['datetime'] = df['datetime']\n",
    "X['hour'] = X['datetime'].dt.hour\n",
    "X['day']   = X['datetime'].dt.day\n",
    "X['month'] = X['datetime'].dt.month\n",
    "X['quarter'] = X['datetime'].dt.quarter\n",
    "X['time'] = (X['datetime'] - X['datetime'].min()).dt.days\n",
    "X['daytime'] = np.where((X['hour'] > 19) & (X['hour'] < 8), True, False)\n",
    "X.drop('datetime', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:8000].copy(), X[8000:].copy(), y[:8000].copy(), y[8000:].copy()\n",
    "X_train, X_val, y_train, y_val = X_train[:6400].copy(), X_train[6400:].copy(), y_train[:6400].copy(), y_train[6400:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7872793165940405"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.fit(X_train, y_train)\n",
    "gbm.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New round of model fitting.  Testing params:  num_trees: 100, learning_rate: 0.001, max_depth: 2\n",
      "Score this round: -0.206620946948727\n",
      "New round of model fitting.  Testing params:  num_trees: 100, learning_rate: 0.001, max_depth: 3\n",
      "Score this round: -0.19445975393114834\n",
      "New round of model fitting.  Testing params:  num_trees: 100, learning_rate: 0.001, max_depth: 4\n",
      "Score this round: -0.18455106971758561\n",
      "New round of model fitting.  Testing params:  num_trees: 100, learning_rate: 0.01, max_depth: 2\n",
      "Score this round: 0.0960298028956702\n",
      "New round of model fitting.  Testing params:  num_trees: 100, learning_rate: 0.01, max_depth: 3\n",
      "Score this round: 0.16661796232319082\n",
      "New round of model fitting.  Testing params:  num_trees: 100, learning_rate: 0.01, max_depth: 4\n",
      "Score this round: 0.2430993989410897\n",
      "New round of model fitting.  Testing params:  num_trees: 100, learning_rate: 0.1, max_depth: 2\n",
      "Score this round: 0.4836991119137151\n",
      "New round of model fitting.  Testing params:  num_trees: 100, learning_rate: 0.1, max_depth: 3\n",
      "Score this round: 0.6455499707905121\n",
      "New round of model fitting.  Testing params:  num_trees: 100, learning_rate: 0.1, max_depth: 4\n",
      "Score this round: 0.746450581785674\n",
      "New round of model fitting.  Testing params:  num_trees: 500, learning_rate: 0.001, max_depth: 2\n",
      "Score this round: -0.030036644956526585\n",
      "New round of model fitting.  Testing params:  num_trees: 500, learning_rate: 0.001, max_depth: 3\n",
      "Score this round: 0.01548324131720902\n",
      "New round of model fitting.  Testing params:  num_trees: 500, learning_rate: 0.001, max_depth: 4\n",
      "Score this round: 0.06515370101825202\n",
      "New round of model fitting.  Testing params:  num_trees: 500, learning_rate: 0.01, max_depth: 2\n",
      "Score this round: 0.40150165439217356\n",
      "New round of model fitting.  Testing params:  num_trees: 500, learning_rate: 0.01, max_depth: 3\n",
      "Score this round: 0.5132452043703355\n",
      "New round of model fitting.  Testing params:  num_trees: 500, learning_rate: 0.01, max_depth: 4\n",
      "Score this round: 0.635951414737022\n",
      "New round of model fitting.  Testing params:  num_trees: 500, learning_rate: 0.1, max_depth: 2\n",
      "Score this round: 0.6542412236824773\n",
      "New round of model fitting.  Testing params:  num_trees: 500, learning_rate: 0.1, max_depth: 3\n",
      "Score this round: 0.7495227165211131\n",
      "New round of model fitting.  Testing params:  num_trees: 500, learning_rate: 0.1, max_depth: 4\n",
      "Score this round: 0.786334132795435\n",
      "New round of model fitting.  Testing params:  num_trees: 1000, learning_rate: 0.001, max_depth: 2\n",
      "Score this round: 0.09494275936493668\n",
      "New round of model fitting.  Testing params:  num_trees: 1000, learning_rate: 0.001, max_depth: 3\n",
      "Score this round: 0.1655557952790081\n",
      "New round of model fitting.  Testing params:  num_trees: 1000, learning_rate: 0.001, max_depth: 4\n",
      "Score this round: 0.2422166848228499\n",
      "New round of model fitting.  Testing params:  num_trees: 1000, learning_rate: 0.01, max_depth: 2\n",
      "Score this round: 0.4825313420338558\n",
      "New round of model fitting.  Testing params:  num_trees: 1000, learning_rate: 0.01, max_depth: 3\n",
      "Score this round: 0.6418234566701091\n",
      "New round of model fitting.  Testing params:  num_trees: 1000, learning_rate: 0.01, max_depth: 4\n",
      "Score this round: 0.7396521580229556\n",
      "New round of model fitting.  Testing params:  num_trees: 1000, learning_rate: 0.1, max_depth: 2\n",
      "Score this round: 0.6574676134037443\n",
      "New round of model fitting.  Testing params:  num_trees: 1000, learning_rate: 0.1, max_depth: 3\n",
      "Score this round: 0.7416981513954395\n",
      "New round of model fitting.  Testing params:  num_trees: 1000, learning_rate: 0.1, max_depth: 4\n",
      "Score this round: 0.7818618469235209\n",
      "New round of model fitting.  Testing params:  num_trees: 5000, learning_rate: 0.001, max_depth: 2\n",
      "Score this round: 0.40084573427920855\n",
      "New round of model fitting.  Testing params:  num_trees: 5000, learning_rate: 0.001, max_depth: 3\n",
      "Score this round: 0.5122940756671776\n",
      "New round of model fitting.  Testing params:  num_trees: 5000, learning_rate: 0.001, max_depth: 4\n",
      "Score this round: 0.6364082004052412\n",
      "New round of model fitting.  Testing params:  num_trees: 5000, learning_rate: 0.01, max_depth: 2\n",
      "Score this round: 0.6592871099309255\n",
      "New round of model fitting.  Testing params:  num_trees: 5000, learning_rate: 0.01, max_depth: 3\n",
      "Score this round: 0.7614734197042557\n",
      "New round of model fitting.  Testing params:  num_trees: 5000, learning_rate: 0.01, max_depth: 4\n",
      "Score this round: 0.7620699649625396\n",
      "New round of model fitting.  Testing params:  num_trees: 5000, learning_rate: 0.1, max_depth: 2\n",
      "Score this round: 0.529380075462702\n",
      "New round of model fitting.  Testing params:  num_trees: 5000, learning_rate: 0.1, max_depth: 3\n",
      "Score this round: 0.7383074978838468\n",
      "New round of model fitting.  Testing params:  num_trees: 5000, learning_rate: 0.1, max_depth: 4\n",
      "Score this round: 0.7706631156227188\n"
     ]
    }
   ],
   "source": [
    "# parameter scoring\n",
    "num_trees = [100, 500, 1000, 5000]\n",
    "learning_rates = [.001, .01, .1]\n",
    "max_depth = [2, 3, 4]\n",
    "cv_scores = []\n",
    "\n",
    "for tree in num_trees:\n",
    "    for rate in learning_rates:\n",
    "        for depth in max_depth:\n",
    "            print(f\"New round of model fitting.  Testing params:  num_trees: {tree}, learning_rate: {rate}, max_depth: {depth}\")\n",
    "            gbm.set_params(n_estimators=tree, learning_rate=rate, max_depth=depth)\n",
    "            gbm.fit(X_train, y_train)\n",
    "            mod_score = gbm.score(X_val, y_val)\n",
    "            print(f\"Score this round: {mod_score}\")\n",
    "            cv_scores.append((mod_score, tree, rate, depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.786334132795435, 500, 0.1, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our max score\n",
    "max(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=4,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll set our model to these parameters\n",
    "gbm.set_params(n_estimators=500, learning_rate=0.1, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hour</td>\n",
       "      <td>0.386736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hours</td>\n",
       "      <td>0.252088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>workingday</td>\n",
       "      <td>0.094127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>time</td>\n",
       "      <td>0.085805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>atemp</td>\n",
       "      <td>0.068591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>temp</td>\n",
       "      <td>0.053244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>humidity</td>\n",
       "      <td>0.036526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weather</td>\n",
       "      <td>0.011226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>windspeed</td>\n",
       "      <td>0.004387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>day</td>\n",
       "      <td>0.002836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>month</td>\n",
       "      <td>0.002585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>holiday</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>season</td>\n",
       "      <td>0.000447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>quarter</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>daytime</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Features  Importances\n",
       "10        hour     0.386736\n",
       "8        hours     0.252088\n",
       "2   workingday     0.094127\n",
       "13        time     0.085805\n",
       "5        atemp     0.068591\n",
       "4         temp     0.053244\n",
       "6     humidity     0.036526\n",
       "3      weather     0.011226\n",
       "7    windspeed     0.004387\n",
       "9          day     0.002836\n",
       "11       month     0.002585\n",
       "1      holiday     0.001306\n",
       "0       season     0.000447\n",
       "12     quarter     0.000095\n",
       "14     daytime     0.000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll look at feature importances as well\n",
    "feats = pd.DataFrame({\n",
    "    'Features': X.columns,\n",
    "    'Importances': gbm.feature_importances_\n",
    "}).sort_values(by='Importances', ascending=False)\n",
    "\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>hours</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>time</th>\n",
       "      <th>daytime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  holiday  workingday  weather  temp   atemp  humidity  windspeed  \\\n",
       "0       1        0           0        0  9.84  14.395        81        0.0   \n",
       "1       1        0           0        0  9.02  13.635        80        0.0   \n",
       "2       1        0           0        0  9.02  13.635        80        0.0   \n",
       "3       1        0           0        0  9.84  14.395        75        0.0   \n",
       "4       1        0           0        0  9.84  14.395        75        0.0   \n",
       "\n",
       "   hours  day  hour  month  quarter  time  daytime  \n",
       "0      0    1     0      1        1     0    False  \n",
       "1      1    1     1      1        1     0    False  \n",
       "2      2    1     2      1        1     0    False  \n",
       "3      3    1     3      1        1     0    False  \n",
       "4      4    1     4      1        1     0    False  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Fit Your Model On ALL of your training data\n",
    "\n",
    "An important step here -- now that you've figured out what columns to include, and what ones to exclude, concatenate your training and validation sets, and fit your model on ALL of your training data.\n",
    "\n",
    "The idea now is that you've found the features that help, you should give your model more samples to infer from.\n",
    "\n",
    "You would use the `pd.concat()` method here.\n",
    "\n",
    "Also -- for good measure, standardize all of your training data before fitting it if you haven't done so already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                          init=None, learning_rate=0.1, loss='ls', max_depth=4,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                          n_iter_no_change=None, presort='deprecated',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer here\n",
    "# we'll create the total training set here:\n",
    "X_train = pd.concat([X_train, X_val])\n",
    "y_train = pd.concat([y_train, y_val])\n",
    "\n",
    "gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Score Your Model on the Test Set\n",
    "\n",
    "Once you've found the best version of your model on your validation set, transform your test set so that it is setup the same way.\n",
    "\n",
    "Ie, if you added a column that improved your validation score, add that same column to your test set.  \n",
    "\n",
    "Remember to standardize your test set using the values from your training set.\n",
    "\n",
    "How close were your validation scores to your test set scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8651051069897694"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your answer here\n",
    "gbm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics\n",
    "\n",
    "Now we'll look at a few different areas of your model to see if there's anything causing our results to be skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10:  Make a prediction on your test set, and calculate the error\n",
    "\n",
    "The error in this case is just the difference between the value for `count` and the value of your prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n",
    "df['Prediction'] = gbm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Error'] = df['count'] - df['Prediction']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
